{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507e4dde-4f8f-45b4-a262-9c7297a7cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"analysis_data\")\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2c5af7-5e9b-49f2-b72f-4199c36588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"analysis_data\")\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, process_name, parameters=None):\n",
    "        self.process_name = process_name\n",
    "        self.parameters = parameters or {}\n",
    "        # Create unique checkpoint file based on parameters\n",
    "        param_hash = hash(str(sorted(self.parameters.items())))\n",
    "        self.checkpoint_file = CHECKPOINT_DIR / f\"{process_name}_{param_hash}_progress.json\"\n",
    "        self.progress = self.load_progress()\n",
    "    \n",
    "    def load_progress(self):\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # Verify parameters match\n",
    "                if data.get(\"parameters\") == self.parameters:\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"Parameters changed for {self.process_name}, starting fresh\")\n",
    "                    return self._create_new_progress()\n",
    "        return self._create_new_progress()\n",
    "    \n",
    "    def _create_new_progress(self):\n",
    "        return {\n",
    "            \"parameters\": self.parameters,\n",
    "            \"completed_chunks\": [], \n",
    "            \"status\": \"not_started\", \n",
    "            \"last_chunk\": -1\n",
    "        }\n",
    "    \n",
    "    def save_progress(self, chunk_info):\n",
    "        self.progress[\"completed_chunks\"].append(chunk_info)\n",
    "        self.progress[\"last_chunk\"] = len(self.progress[\"completed_chunks\"]) - 1\n",
    "        self.progress[\"status\"] = \"in_progress\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def mark_completed(self):\n",
    "        self.progress[\"status\"] = \"completed\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def is_chunk_completed(self, chunk_id):\n",
    "        return any(chunk[\"chunk_id\"] == chunk_id for chunk in self.progress[\"completed_chunks\"])\n",
    "    \n",
    "    def get_completed_chunks(self):\n",
    "        return [chunk[\"filename\"] for chunk in self.progress[\"completed_chunks\"]]\n",
    "\n",
    "def save_dataframe(df, filename_base):\n",
    "    \"\"\"Save dataframe with best available format\"\"\"\n",
    "    try:\n",
    "        # Try parquet first\n",
    "        parquet_file = DATA_DIR / f\"{filename_base}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        return parquet_file\n",
    "    except ImportError:\n",
    "        # Fallback to pickle\n",
    "        pickle_file = DATA_DIR / f\"{filename_base}.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        return pickle_file\n",
    "\n",
    "def load_dataframe(filename_base):\n",
    "    \"\"\"Load dataframe from best available format\"\"\"\n",
    "    for ext, loader in [('.parquet', pd.read_parquet), ('.pkl', lambda x: pickle.load(open(x, 'rb')))]:\n",
    "        file_path = DATA_DIR / f\"{filename_base}{ext}\"\n",
    "        if file_path.exists():\n",
    "            return loader(file_path)\n",
    "    return None\n",
    "\n",
    "def connect_server(ip, database):\n",
    "    conn_str = (\n",
    "        \"Driver={SQL Server};\"\n",
    "        \"Server=\" + ip + \";\" + \n",
    "        \"Database=\" + database + \";\" + \n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor\n",
    "\n",
    "def sql_to_pandas(conn, cursor, sql_string):\n",
    "    cursor.execute(sql_string)\n",
    "    rows = cursor.fetchall()\n",
    "    if rows:\n",
    "        return pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def close_conn(conn, cursor):\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0187b676-e0d7-4e3c-8773-c52150bb5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USAGE - CORRECTED VERSION WITH CLIENT_ID AS BANK CARD NUMBER\n",
      "\n",
      "KEY CORRECTION:\n",
      "- reference_number is ONLY used to link facts and bank tables\n",
      "- client_id is the actual bank card number\n",
      "- All analysis and outputs are based on customer_id + client_id pairs\n",
      "\n",
      "RUN ANALYSIS:\n",
      ">>> results, summary = run_full_analysis()\n",
      "\n",
      "OUTPUT FILES:\n",
      "1. customer_card_pairs.csv - PRIMARY OUTPUT\n",
      "   - Simple table: customer_id, client_id, bank_name\n",
      "   - Each customer has 1 or 2 rows (cards)\n",
      "   - If 2 cards, they are from different banks\n",
      "\n",
      "2. final_clean_customers_summary.csv\n",
      "   - One row per customer with aggregated information\n",
      "   \n",
      "3. customer_card_combinations_detailed.csv\n",
      "   - Full details including spending, cheque count, etc.\n",
      "\n",
      "CONSTRAINTS ENFORCED:\n",
      "- Each customer has <=2 client_ids (bank cards) from <=2 banks\n",
      "- If 2 cards, they must be from 2 different banks (BOG or TBC)\n",
      "- Only cards with >=3 transactions OR >$35 spending\n",
      "- Shared cards (used by multiple customers) are removed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"analysis_data\")\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, process_name, parameters=None):\n",
    "        self.process_name = process_name\n",
    "        self.parameters = parameters or {}\n",
    "        param_hash = hash(str(sorted(self.parameters.items())))\n",
    "        self.checkpoint_file = CHECKPOINT_DIR / f\"{process_name}_{param_hash}_progress.json\"\n",
    "        self.progress = self.load_progress()\n",
    "    \n",
    "    def load_progress(self):\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if data.get(\"parameters\") == self.parameters:\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"Parameters changed for {self.process_name}, starting fresh\")\n",
    "                    return self._create_new_progress()\n",
    "        return self._create_new_progress()\n",
    "    \n",
    "    def _create_new_progress(self):\n",
    "        return {\n",
    "            \"parameters\": self.parameters,\n",
    "            \"completed_chunks\": [], \n",
    "            \"status\": \"not_started\", \n",
    "            \"last_chunk\": -1\n",
    "        }\n",
    "    \n",
    "    def save_progress(self, chunk_info):\n",
    "        self.progress[\"completed_chunks\"].append(chunk_info)\n",
    "        self.progress[\"last_chunk\"] = len(self.progress[\"completed_chunks\"]) - 1\n",
    "        self.progress[\"status\"] = \"in_progress\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def mark_completed(self):\n",
    "        self.progress[\"status\"] = \"completed\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def is_chunk_completed(self, chunk_id):\n",
    "        return any(chunk[\"chunk_id\"] == chunk_id for chunk in self.progress[\"completed_chunks\"])\n",
    "    \n",
    "    def get_completed_chunks(self):\n",
    "        return [chunk[\"filename\"] for chunk in self.progress[\"completed_chunks\"]]\n",
    "\n",
    "def save_dataframe(df, filename_base):\n",
    "    \"\"\"Save dataframe with best available format\"\"\"\n",
    "    try:\n",
    "        parquet_file = DATA_DIR / f\"{filename_base}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        return parquet_file\n",
    "    except ImportError:\n",
    "        pickle_file = DATA_DIR / f\"{filename_base}.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        return pickle_file\n",
    "\n",
    "def load_dataframe(filename_base):\n",
    "    \"\"\"Load dataframe from best available format\"\"\"\n",
    "    for ext, loader in [('.parquet', pd.read_parquet), ('.pkl', lambda x: pickle.load(open(x, 'rb')))]:\n",
    "        file_path = DATA_DIR / f\"{filename_base}{ext}\"\n",
    "        if file_path.exists():\n",
    "            return loader(file_path)\n",
    "    return None\n",
    "\n",
    "def connect_server(ip, database):\n",
    "    conn_str = (\n",
    "        \"Driver={SQL Server};\"\n",
    "        \"Server=\" + ip + \";\" + \n",
    "        \"Database=\" + database + \";\" + \n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor\n",
    "\n",
    "def sql_to_pandas(conn, cursor, sql_string):\n",
    "    cursor.execute(sql_string)\n",
    "    rows = cursor.fetchall()\n",
    "    if rows:\n",
    "        return pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def close_conn(conn, cursor):\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def analyze_loyalty_bank_cards_corrected(facts_df, bank_df):\n",
    "    \"\"\"\n",
    "    CORRECTED analysis using client_id as bank card identifier\n",
    "    \n",
    "    Key changes:\n",
    "    - reference_number is ONLY used to link tables\n",
    "    - client_id is the actual bank card number\n",
    "    - All grouping and filtering is done by customer_id + client_id\n",
    "    \n",
    "    Logic:\n",
    "    1. Aggregate facts by cheque_id to get cheque totals\n",
    "    2. Join with bank_df using reference_number to get client_id\n",
    "    3. Validate each customer_id + client_id combination (>=3 cheques OR >$35 total)\n",
    "    4. Keep customers with <=2 different client_ids from <=2 different banks\n",
    "    5. If customer has 2 client_ids, they MUST be from 2 different banks\n",
    "    6. Remove shared client_ids (used by multiple customers)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CORRECTED LOYALTY BANK CARD ANALYSIS (using client_id) ===\")\n",
    "    \n",
    "    if facts_df is None or bank_df is None or facts_df.empty or bank_df.empty:\n",
    "        print(\"Invalid input data\")\n",
    "        return None, None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 0: AGGREGATE FACTS DATA BY CHEQUE_ID\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 0: AGGREGATING FACTS DATA BY CHEQUE ---\")\n",
    "    \n",
    "    print(f\"Raw transaction lines: {len(facts_df):,}\")\n",
    "    print(f\"Unique cheques: {facts_df['cheque_id'].nunique():,}\")\n",
    "    print(f\"Unique customers: {facts_df['customer_id'].nunique():,}\")\n",
    "    print(f\"Unique reference numbers: {facts_df['reference_number'].nunique():,}\")\n",
    "    \n",
    "    cheque_totals = facts_df.groupby(['cheque_id', 'customer_id', 'reference_number']).agg({\n",
    "        'total_price': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    cheque_totals.columns = ['cheque_id', 'customer_id', 'reference_number', 'cheque_total']\n",
    "    \n",
    "    print(f\"Cheque-level data: {len(cheque_totals):,} records\")\n",
    "    print(f\"Average spending per cheque: ${cheque_totals['cheque_total'].mean():.2f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: JOIN WITH BANK DATA TO GET CLIENT_ID (BANK CARD NUMBER)\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 1: JOINING WITH BANK DATA TO GET CLIENT_ID ---\")\n",
    "    \n",
    "    cheque_totals['reference_number'] = cheque_totals['reference_number'].astype(str)\n",
    "    bank_df['reference_number'] = bank_df['reference_number'].astype(str)\n",
    "    \n",
    "    # Merge using reference_number to get client_id\n",
    "    merged_df = cheque_totals.merge(\n",
    "        bank_df[['reference_number', 'client_id', 'bank_name', 'client']], \n",
    "        on='reference_number', \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"After join:\")\n",
    "    print(f\"  Successfully merged cheque records: {len(merged_df):,}\")\n",
    "    print(f\"  Unique customers with bank data: {merged_df['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique bank cards (client_id): {merged_df['client_id'].nunique():,}\")\n",
    "    print(f\"  Unique reference numbers used for linking: {merged_df['reference_number'].nunique():,}\")\n",
    "    print(f\"  Available banks: {sorted(merged_df['bank_name'].unique())}\")\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        print(\"No matching data found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: VALIDATE CUSTOMER-CARD COMBINATIONS (using client_id)\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 2: VALIDATING CUSTOMER-CARD COMBINATIONS ---\")\n",
    "    print(\"Validation: customer_id must have >=3 cheques OR >$35 total spending per client_id\")\n",
    "    \n",
    "    # Group by customer_id + client_id to get usage stats\n",
    "    customer_card_usage = merged_df.groupby(['customer_id', 'client_id']).agg({\n",
    "        'cheque_id': 'nunique',\n",
    "        'cheque_total': 'sum',\n",
    "        'bank_name': 'first',\n",
    "        'client': 'first',\n",
    "        'reference_number': lambda x: list(x.unique())  # Track which reference numbers link to this client_id\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_card_usage.columns = ['customer_id', 'client_id', 'cheque_count', \n",
    "                                    'total_spending', 'bank_name', 'client', 'reference_numbers']\n",
    "    \n",
    "    print(f\"Customer-card combinations: {len(customer_card_usage):,}\")\n",
    "    print(f\"Average cheques per combination: {customer_card_usage['cheque_count'].mean():.1f}\")\n",
    "    print(f\"Average spending per combination: ${customer_card_usage['total_spending'].mean():.2f}\")\n",
    "    \n",
    "    # Apply validation: >=3 cheques OR >$35 spending\n",
    "    valid_customer_cards = customer_card_usage[\n",
    "        (customer_card_usage['cheque_count'] >= 3) | \n",
    "        (customer_card_usage['total_spending'] > 35)\n",
    "    ].copy()\n",
    "    \n",
    "    invalid_customer_cards = customer_card_usage[\n",
    "        (customer_card_usage['cheque_count'] < 3) & \n",
    "        (customer_card_usage['total_spending'] <= 35)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Valid customer-card combinations: {len(valid_customer_cards):,}\")\n",
    "    print(f\"Invalid customer-card combinations: {len(invalid_customer_cards):,}\")\n",
    "    print(f\"Validation rate: {len(valid_customer_cards)/len(customer_card_usage)*100:.1f}%\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: FILTER TO BOG AND TBC BANKS ONLY\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 3: FILTERING TO BOG AND TBC BANKS ---\")\n",
    "    \n",
    "    target_banks = ['BOG', 'TBC']\n",
    "    bog_tbc_valid_cards = valid_customer_cards[\n",
    "        valid_customer_cards['bank_name'].isin(target_banks)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Valid combinations with BOG/TBC banks: {len(bog_tbc_valid_cards):,}\")\n",
    "    print(\"Bank distribution:\")\n",
    "    bank_dist = bog_tbc_valid_cards['bank_name'].value_counts()\n",
    "    for bank, count in bank_dist.items():\n",
    "        print(f\"  {bank}: {count:,} combinations\")\n",
    "    \n",
    "    if len(bog_tbc_valid_cards) == 0:\n",
    "        print(\"No valid BOG/TBC combinations found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: FILTER CUSTOMERS WITH <=2 VALID CARDS FROM <=2 BANKS\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 4: FILTERING CUSTOMERS BY CARD AND BANK LIMITS ---\")\n",
    "    print(\"Requirements:\")\n",
    "    print(\"  - <=2 different valid client_ids from <=2 different banks\")\n",
    "    print(\"  - If 2 client_ids, they MUST be from 2 different banks\")\n",
    "    \n",
    "    customer_summary = bog_tbc_valid_cards.groupby('customer_id').agg({\n",
    "        'client_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': ['nunique', lambda x: list(x.unique())],\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_summary.columns = ['customer_id', 'valid_cards_count', 'valid_cards_list',\n",
    "                                'different_banks_count', 'different_banks_list', \n",
    "                                'total_cheques', 'total_spending']\n",
    "    \n",
    "    print(f\"Customers with valid BOG/TBC cards: {len(customer_summary):,}\")\n",
    "    \n",
    "    # Apply filtering with constraints\n",
    "    eligible_customers = customer_summary[\n",
    "        (customer_summary['valid_cards_count'] <= 2) & \n",
    "        (customer_summary['different_banks_count'] <= 2) &\n",
    "        (\n",
    "            (customer_summary['valid_cards_count'] == 1) |\n",
    "            ((customer_summary['valid_cards_count'] == 2) & (customer_summary['different_banks_count'] == 2))\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"After filtering: {len(eligible_customers):,} customers meeting requirements\")\n",
    "    \n",
    "    if len(eligible_customers) == 0:\n",
    "        print(\"No customers meet the criteria!\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"\\nValid cards per customer distribution:\")\n",
    "    cards_after = eligible_customers['valid_cards_count'].value_counts().sort_index()\n",
    "    for cards, count in cards_after.items():\n",
    "        print(f\"  {count:,} customers have {cards} valid card(s)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: REMOVE SHARED CLIENT_IDS\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 5: REMOVING SHARED CLIENT_IDS ---\")\n",
    "    print(\"Remove client_ids used by multiple customers, but keep customers with remaining cards\")\n",
    "    \n",
    "    eligible_customer_ids = set(eligible_customers['customer_id'])\n",
    "    eligible_customer_cards = bog_tbc_valid_cards[\n",
    "        bog_tbc_valid_cards['customer_id'].isin(eligible_customer_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    # Check how many customers use each client_id\n",
    "    card_usage_analysis = eligible_customer_cards.groupby('client_id').agg({\n",
    "        'customer_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': 'first',\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    card_usage_analysis.columns = ['client_id', 'customer_count', 'customer_list',\n",
    "                                   'bank_name', 'total_cheques', 'total_spending']\n",
    "    \n",
    "    shared_cards = card_usage_analysis[\n",
    "        card_usage_analysis['customer_count'] > 1\n",
    "    ]['client_id'].tolist()\n",
    "    \n",
    "    unique_cards = card_usage_analysis[\n",
    "        card_usage_analysis['customer_count'] == 1\n",
    "    ]['client_id'].tolist()\n",
    "    \n",
    "    print(f\"Cards (client_ids) used by single customer: {len(unique_cards):,}\")\n",
    "    print(f\"Cards (client_ids) shared by multiple customers: {len(shared_cards):,}\")\n",
    "    \n",
    "    # Remove shared cards\n",
    "    clean_customer_cards = eligible_customer_cards[\n",
    "        ~eligible_customer_cards['client_id'].isin(shared_cards)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Customer-card combinations after removing shared cards: {len(clean_customer_cards):,}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: CREATE FINAL CLEAN DATASET\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- STEP 6: CREATING FINAL CLEAN DATASET ---\")\n",
    "    \n",
    "    final_customer_summary = clean_customer_cards.groupby('customer_id').agg({\n",
    "        'client_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': ['nunique', lambda x: list(x.unique())],\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum',\n",
    "        'client': lambda x: list(x.unique()),\n",
    "        'reference_numbers': lambda x: list(set([ref for refs in x for ref in refs]))\n",
    "    }).reset_index()\n",
    "    \n",
    "    final_customer_summary.columns = ['customer_id', 'final_cards_count', 'final_cards_list',\n",
    "                                      'different_banks_count', 'different_banks_list', \n",
    "                                      'total_cheques', 'total_spending', 'clients_list', 'reference_numbers']\n",
    "    \n",
    "    final_customer_summary = final_customer_summary[\n",
    "        final_customer_summary['final_cards_count'] > 0\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Final clean customers: {len(final_customer_summary):,}\")\n",
    "    \n",
    "    # Verify constraints\n",
    "    constraint_violations = final_customer_summary[\n",
    "        (final_customer_summary['final_cards_count'] == 2) & \n",
    "        (final_customer_summary['different_banks_count'] != 2)\n",
    "    ]\n",
    "    \n",
    "    if len(constraint_violations) > 0:\n",
    "        print(f\"WARNING: {len(constraint_violations)} customers have 2 cards from same bank\")\n",
    "        final_customer_summary = final_customer_summary[\n",
    "            ~final_customer_summary['customer_id'].isin(constraint_violations['customer_id'])\n",
    "        ].copy()\n",
    "        print(f\"Removed violations, final customers: {len(final_customer_summary):,}\")\n",
    "    \n",
    "    print(\"\\nFINAL DISTRIBUTION - Cards per customer:\")\n",
    "    final_cards_dist = final_customer_summary['final_cards_count'].value_counts().sort_index()\n",
    "    for cards, count in final_cards_dist.items():\n",
    "        print(f\"  {count:,} customers have {cards} card(s)\")\n",
    "    \n",
    "    # Final verification\n",
    "    max_cards = final_customer_summary['final_cards_count'].max()\n",
    "    max_banks = final_customer_summary['different_banks_count'].max()\n",
    "    two_cards_same_bank = len(final_customer_summary[\n",
    "        (final_customer_summary['final_cards_count'] == 2) & \n",
    "        (final_customer_summary['different_banks_count'] == 1)\n",
    "    ])\n",
    "    \n",
    "    # Check for shared cards in final\n",
    "    final_customer_ids = set(final_customer_summary['customer_id'])\n",
    "    final_clean_cards = clean_customer_cards[\n",
    "        clean_customer_cards['customer_id'].isin(final_customer_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    final_card_sharing = final_clean_cards.groupby('client_id')['customer_id'].nunique()\n",
    "    shared_in_final = (final_card_sharing > 1).sum()\n",
    "    \n",
    "    print(f\"\\nFINAL VERIFICATION:\")\n",
    "    print(f\"  Maximum cards per customer: {max_cards} (requirement: <=2) {'PASS' if max_cards <= 2 else 'FAIL'}\")\n",
    "    print(f\"  Maximum banks per customer: {max_banks} (requirement: <=2) {'PASS' if max_banks <= 2 else 'FAIL'}\")\n",
    "    print(f\"  Customers with 2 cards from same bank: {two_cards_same_bank} (requirement: 0) {'PASS' if two_cards_same_bank == 0 else 'FAIL'}\")\n",
    "    print(f\"  Shared cards in final dataset: {shared_in_final} (requirement: 0) {'PASS' if shared_in_final == 0 else 'FAIL'}\")\n",
    "    \n",
    "    all_passed = max_cards <= 2 and max_banks <= 2 and two_cards_same_bank == 0 and shared_in_final == 0\n",
    "    print(f\"  OVERALL: {'ALL REQUIREMENTS SATISFIED' if all_passed else 'REQUIREMENTS NOT MET'}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE CUSTOMER-CARD PAIRS OUTPUT\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- CREATING CUSTOMER-CARD PAIRS ---\")\n",
    "    \n",
    "    # Create simple pairs dataframe\n",
    "    customer_card_pairs = final_clean_cards[['customer_id', 'client_id', 'bank_name']].drop_duplicates()\n",
    "    print(f\"Total customer-card pairs: {len(customer_card_pairs):,}\")\n",
    "    \n",
    "    # Verify each customer has 1 or 2 cards\n",
    "    pairs_per_customer = customer_card_pairs.groupby('customer_id').size()\n",
    "    print(f\"Pairs per customer: min={pairs_per_customer.min()}, max={pairs_per_customer.max()}\")\n",
    "    \n",
    "    results = {\n",
    "        'raw_transaction_lines': len(facts_df),\n",
    "        'total_cheques': len(cheque_totals),\n",
    "        'total_loyalty_customers': cheque_totals['customer_id'].nunique(),\n",
    "        'customers_with_bank_data': merged_df['customer_id'].nunique(),\n",
    "        'total_customer_card_combinations': len(customer_card_usage),\n",
    "        'valid_customer_card_combinations': len(valid_customer_cards),\n",
    "        'bog_tbc_valid_combinations': len(bog_tbc_valid_cards),\n",
    "        'eligible_customers_before_shared_removal': len(eligible_customers),\n",
    "        'shared_cards_count': len(shared_cards),\n",
    "        'final_clean_customers': len(final_customer_summary),\n",
    "        'final_clean_combinations': len(final_clean_cards),\n",
    "        \n",
    "        # DataFrames\n",
    "        'cheque_totals': cheque_totals,\n",
    "        'customer_card_usage': customer_card_usage,\n",
    "        'valid_customer_cards': valid_customer_cards,\n",
    "        'invalid_customer_cards': invalid_customer_cards,\n",
    "        'bog_tbc_valid_cards': bog_tbc_valid_cards,\n",
    "        'eligible_customers': eligible_customers,\n",
    "        'card_usage_analysis': card_usage_analysis,\n",
    "        'shared_cards_info': card_usage_analysis[card_usage_analysis['customer_count'] > 1],\n",
    "        'shared_cards_list': shared_cards,\n",
    "        'final_clean_cards': final_clean_cards,\n",
    "        'final_customer_summary': final_customer_summary,\n",
    "        'customer_card_pairs': customer_card_pairs,  # NEW: Simple pairs output\n",
    "        'merged_data': merged_df,\n",
    "        \n",
    "        # Distributions\n",
    "        'final_cards_distribution': final_cards_dist\n",
    "    }\n",
    "    \n",
    "    return results, final_customer_summary\n",
    "\n",
    "def save_corrected_results(results, output_dir=\"./data\"):\n",
    "    \"\"\"Save all results to CSV files with focus on customer-card pairs\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    try:\n",
    "        # PRIMARY OUTPUT: Customer-Card Pairs\n",
    "        if 'customer_card_pairs' in results and not results['customer_card_pairs'].empty:\n",
    "            file_path = output_path / \"customer_card_pairs.csv\"\n",
    "            results['customer_card_pairs'].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "            print(f\"\\nPRIMARY OUTPUT SAVED: {file_path}\")\n",
    "            print(f\"  Rows: {len(results['customer_card_pairs']):,}\")\n",
    "            print(f\"  Columns: {list(results['customer_card_pairs'].columns)}\")\n",
    "        \n",
    "        # SECONDARY OUTPUT: Customer summary with all details\n",
    "        if 'final_customer_summary' in results and not results['final_customer_summary'].empty:\n",
    "            summary_csv = results['final_customer_summary'].copy()\n",
    "            \n",
    "            # Convert list columns to strings\n",
    "            summary_csv['final_cards_list_str'] = summary_csv['final_cards_list'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            summary_csv['different_banks_list_str'] = summary_csv['different_banks_list'].apply(\n",
    "                lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "            summary_csv['clients_list_str'] = summary_csv['clients_list'].apply(\n",
    "                lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "            summary_csv['reference_numbers_str'] = summary_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x[:20])) + (f' (+{len(x)-20} more)' if len(x) > 20 else '') if isinstance(x, list) else str(x))\n",
    "            \n",
    "            csv_columns = ['customer_id', 'final_cards_count', 'different_banks_count', 'total_cheques', \n",
    "                          'total_spending', 'final_cards_list_str', 'different_banks_list_str', \n",
    "                          'clients_list_str', 'reference_numbers_str']\n",
    "            \n",
    "            file_path = output_path / \"final_clean_customers_summary.csv\"\n",
    "            summary_csv[csv_columns].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # DETAILED OUTPUT: All customer-card combinations with full details\n",
    "        if 'final_clean_cards' in results and not results['final_clean_cards'].empty:\n",
    "            clean_cards_csv = results['final_clean_cards'].copy()\n",
    "            clean_cards_csv['reference_numbers_str'] = clean_cards_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            output_cols = ['customer_id', 'client_id', 'bank_name', 'cheque_count', \n",
    "                          'total_spending', 'client', 'reference_numbers_str']\n",
    "            \n",
    "            file_path = output_path / \"customer_card_combinations_detailed.csv\"\n",
    "            clean_cards_csv[output_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Shared cards that were removed\n",
    "        if 'shared_cards_info' in results and not results['shared_cards_info'].empty:\n",
    "            file_path = output_path / \"shared_cards_removed.csv\"\n",
    "            shared_csv = results['shared_cards_info'].copy()\n",
    "            shared_csv['customer_list_str'] = shared_csv['customer_list'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            csv_cols = ['client_id', 'customer_count', 'bank_name', 'total_cheques', \n",
    "                       'total_spending', 'customer_list_str']\n",
    "            shared_csv[csv_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Invalid customer-card combinations\n",
    "        if 'invalid_customer_cards' in results and not results['invalid_customer_cards'].empty:\n",
    "            file_path = output_path / \"invalid_customer_card_combinations.csv\"\n",
    "            invalid_csv = results['invalid_customer_cards'].copy()\n",
    "            invalid_csv['reference_numbers_str'] = invalid_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            output_cols = ['customer_id', 'client_id', 'cheque_count', 'total_spending', \n",
    "                          'bank_name', 'reference_numbers_str']\n",
    "            invalid_csv[output_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Analysis summary\n",
    "        analysis_summary = {\n",
    "            'total_loyalty_customers': results['total_loyalty_customers'],\n",
    "            'final_clean_customers': results['final_clean_customers'],\n",
    "            'success_rate_percent': results['final_clean_customers']/results['total_loyalty_customers']*100,\n",
    "            'shared_cards_removed': results['shared_cards_count'],\n",
    "            'validation_rate_percent': results['valid_customer_card_combinations']/results['total_customer_card_combinations']*100\n",
    "        }\n",
    "        \n",
    "        file_path = output_path / \"analysis_summary.csv\"\n",
    "        pd.DataFrame([analysis_summary]).to_csv(file_path, index=False)\n",
    "        saved_files.append(str(file_path))\n",
    "        \n",
    "        print(f\"\\nSAVED {len(saved_files)} FILES:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"   {file_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def run_full_analysis():\n",
    "    \"\"\"Run complete analysis\"\"\"\n",
    "    print(\"RUNNING COMPLETE ANALYSIS - WITH CLIENT_ID\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    facts_df = load_dataframe(\"facts_data_2025_combined_customer_id\")\n",
    "    bank_df = load_dataframe(\"bank_data_combined\")\n",
    "    \n",
    "    if facts_df is None or bank_df is None:\n",
    "        print(\"No saved data found, loading from database...\")\n",
    "        # Add your data loading logic here\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Using existing saved data\")\n",
    "        print(f\"Facts data: {len(facts_df):,} records\")\n",
    "        print(f\"Bank data: {len(bank_df):,} records\")\n",
    "    \n",
    "    if facts_df is None or bank_df is None:\n",
    "        print(\"Could not load data\")\n",
    "        return None\n",
    "    \n",
    "    # Run analysis\n",
    "    results, summary = analyze_loyalty_bank_cards_corrected(facts_df, bank_df)\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"Analysis failed\")\n",
    "        return None\n",
    "    \n",
    "    # Save results\n",
    "    save_corrected_results(results)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "def show_usage():\n",
    "    print(\"\"\"\n",
    "USAGE - CORRECTED VERSION WITH CLIENT_ID AS BANK CARD NUMBER\n",
    "\n",
    "KEY CORRECTION:\n",
    "- reference_number is ONLY used to link facts and bank tables\n",
    "- client_id is the actual bank card number\n",
    "- All analysis and outputs are based on customer_id + client_id pairs\n",
    "\n",
    "RUN ANALYSIS:\n",
    ">>> results, summary = run_full_analysis()\n",
    "\n",
    "OUTPUT FILES:\n",
    "1. customer_card_pairs.csv - PRIMARY OUTPUT\n",
    "   - Simple table: customer_id, client_id, bank_name\n",
    "   - Each customer has 1 or 2 rows (cards)\n",
    "   - If 2 cards, they are from different banks\n",
    "\n",
    "2. final_clean_customers_summary.csv\n",
    "   - One row per customer with aggregated information\n",
    "   \n",
    "3. customer_card_combinations_detailed.csv\n",
    "   - Full details including spending, cheque count, etc.\n",
    "\n",
    "CONSTRAINTS ENFORCED:\n",
    "- Each customer has <=2 client_ids (bank cards) from <=2 banks\n",
    "- If 2 cards, they must be from 2 different banks (BOG or TBC)\n",
    "- Only cards with >=3 transactions OR >$35 spending\n",
    "- Shared cards (used by multiple customers) are removed\n",
    "\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    show_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434fa3a8-6334-4f10-82b2-19ef85c40793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USAGE - CORRECTED VERSION WITH CLIENT AS BANK CARD NUMBER\n",
      "\n",
      "==================================================================\n",
      "VIEW YOUR DATA FIRST (RECOMMENDED)\n",
      "==================================================================\n",
      ">>> bank_df = view_bank_data()\n",
      ">>> facts_df = view_facts_data()\n",
      "\n",
      "==================================================================\n",
      "RUN ANALYSIS\n",
      "==================================================================\n",
      "Option 1 - Use presaved data:\n",
      ">>> results, summary = run_full_analysis()\n",
      "\n",
      "Option 2 - Use dataframes in memory:\n",
      ">>> results, summary = run_analysis_with_dataframes(facts_df, bank_df)\n",
      "\n",
      "==================================================================\n",
      "OUTPUT FILES\n",
      "==================================================================\n",
      "Primary: customer_card_pairs.csv (customer_id, bank_card_id, bank_name)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"analysis_data\")\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, process_name, parameters=None):\n",
    "        self.process_name = process_name\n",
    "        self.parameters = parameters or {}\n",
    "        param_hash = hash(str(sorted(self.parameters.items())))\n",
    "        self.checkpoint_file = CHECKPOINT_DIR / f\"{process_name}_{param_hash}_progress.json\"\n",
    "        self.progress = self.load_progress()\n",
    "    \n",
    "    def load_progress(self):\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if data.get(\"parameters\") == self.parameters:\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"Parameters changed for {self.process_name}, starting fresh\")\n",
    "                    return self._create_new_progress()\n",
    "        return self._create_new_progress()\n",
    "    \n",
    "    def _create_new_progress(self):\n",
    "        return {\n",
    "            \"parameters\": self.parameters,\n",
    "            \"completed_chunks\": [], \n",
    "            \"status\": \"not_started\", \n",
    "            \"last_chunk\": -1\n",
    "        }\n",
    "    \n",
    "    def save_progress(self, chunk_info):\n",
    "        self.progress[\"completed_chunks\"].append(chunk_info)\n",
    "        self.progress[\"last_chunk\"] = len(self.progress[\"completed_chunks\"]) - 1\n",
    "        self.progress[\"status\"] = \"in_progress\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def mark_completed(self):\n",
    "        self.progress[\"status\"] = \"completed\"\n",
    "        self.progress[\"parameters\"] = self.parameters\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def is_chunk_completed(self, chunk_id):\n",
    "        return any(chunk[\"chunk_id\"] == chunk_id for chunk in self.progress[\"completed_chunks\"])\n",
    "    \n",
    "    def get_completed_chunks(self):\n",
    "        return [chunk[\"filename\"] for chunk in self.progress[\"completed_chunks\"]]\n",
    "\n",
    "def save_dataframe(df, filename_base):\n",
    "    \"\"\"Save dataframe with best available format\"\"\"\n",
    "    try:\n",
    "        parquet_file = DATA_DIR / f\"{filename_base}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        return parquet_file\n",
    "    except ImportError:\n",
    "        pickle_file = DATA_DIR / f\"{filename_base}.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        return pickle_file\n",
    "\n",
    "def load_dataframe(filename_base):\n",
    "    \"\"\"Load dataframe from best available format\"\"\"\n",
    "    for ext, loader in [('.parquet', pd.read_parquet), ('.pkl', lambda x: pickle.load(open(x, 'rb')))]:\n",
    "        file_path = DATA_DIR / f\"{filename_base}{ext}\"\n",
    "        if file_path.exists():\n",
    "            return loader(file_path)\n",
    "    return None\n",
    "\n",
    "def connect_server(ip, database):\n",
    "    conn_str = (\n",
    "        \"Driver={SQL Server};\"\n",
    "        \"Server=\" + ip + \";\" + \n",
    "        \"Database=\" + database + \";\" + \n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor\n",
    "\n",
    "def sql_to_pandas(conn, cursor, sql_string):\n",
    "    cursor.execute(sql_string)\n",
    "    rows = cursor.fetchall()\n",
    "    if rows:\n",
    "        return pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def close_conn(conn, cursor):\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def view_bank_data(num_rows=10):\n",
    "    \"\"\"View sample of your bank data\"\"\"\n",
    "    print(\"LOADING BANK DATA...\")\n",
    "    bank_df = load_dataframe(\"bank_data_combined\")\n",
    "    \n",
    "    if bank_df is None:\n",
    "        print(\"ERROR: Could not find bank_data_combined.parquet or .pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nBANK DATA OVERVIEW:\")\n",
    "    print(f\"  Total rows: {len(bank_df):,}\")\n",
    "    print(f\"  Columns: {list(bank_df.columns)}\")\n",
    "    print(f\"\\nColumn types:\")\n",
    "    for col, dtype in bank_df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    print(f\"\\nFirst {num_rows} rows:\")\n",
    "    print(bank_df.head(num_rows))\n",
    "    \n",
    "    print(f\"\\nUnique values per column:\")\n",
    "    for col in bank_df.columns:\n",
    "        unique_count = bank_df[col].nunique()\n",
    "        print(f\"  {col}: {unique_count:,} unique values\")\n",
    "    \n",
    "    if 'bank_name' in bank_df.columns:\n",
    "        print(f\"\\nBank distribution:\")\n",
    "        print(bank_df['bank_name'].value_counts())\n",
    "    \n",
    "    return bank_df\n",
    "\n",
    "def view_facts_data(num_rows=10):\n",
    "    \"\"\"View sample of your facts data\"\"\"\n",
    "    print(\"LOADING FACTS DATA...\")\n",
    "    facts_df = load_dataframe(\"facts_data_2025_combined_customer_id\")\n",
    "    \n",
    "    if facts_df is None:\n",
    "        print(\"ERROR: Could not find facts_data_2025_combined_customer_id.parquet or .pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nFACTS DATA OVERVIEW:\")\n",
    "    print(f\"  Total rows: {len(facts_df):,}\")\n",
    "    print(f\"  Columns: {list(facts_df.columns)}\")\n",
    "    print(f\"\\nColumn types:\")\n",
    "    for col, dtype in facts_df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    print(f\"\\nFirst {num_rows} rows:\")\n",
    "    print(facts_df.head(num_rows))\n",
    "    \n",
    "    print(f\"\\nKey metrics:\")\n",
    "    print(f\"  Unique customers: {facts_df['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique cheques: {facts_df['cheque_id'].nunique():,}\")\n",
    "    print(f\"  Unique reference numbers: {facts_df['reference_number'].nunique():,}\")\n",
    "    \n",
    "    return facts_df\n",
    "\n",
    "def analyze_loyalty_bank_cards_corrected(facts_df, bank_df):\n",
    "    \"\"\"\n",
    "    CORRECTED analysis using client as bank card identifier\n",
    "    \n",
    "    Key changes:\n",
    "    - reference_number is ONLY used to link tables\n",
    "    - client (or client_id) is the actual bank card number\n",
    "    - All grouping and filtering is done by customer_id + bank_card_id\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CORRECTED LOYALTY BANK CARD ANALYSIS ===\")\n",
    "    \n",
    "    if facts_df is None or bank_df is None or facts_df.empty or bank_df.empty:\n",
    "        print(\"Invalid input data\")\n",
    "        return None, None\n",
    "    \n",
    "    # DIAGNOSTIC: Check what columns we actually have\n",
    "    print(\"\\n--- DIAGNOSTIC: CHECKING AVAILABLE COLUMNS ---\")\n",
    "    print(f\"Facts columns: {list(facts_df.columns)}\")\n",
    "    print(f\"Bank columns: {list(bank_df.columns)}\")\n",
    "    print(f\"\\nBank data sample (first row):\")\n",
    "    print(bank_df.head(1).to_dict('records'))\n",
    "    \n",
    "    # Determine the correct column name for bank card identifier\n",
    "    bank_card_column = None\n",
    "    if 'client_id' in bank_df.columns:\n",
    "        bank_card_column = 'client_id'\n",
    "    elif 'client' in bank_df.columns:\n",
    "        bank_card_column = 'client'\n",
    "    else:\n",
    "        print(\"\\nERROR: Cannot find bank card identifier column!\")\n",
    "        print(\"Looking for 'client_id' or 'client' in bank_df columns\")\n",
    "        print(f\"Available columns: {list(bank_df.columns)}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nUsing '{bank_card_column}' as bank card identifier\")\n",
    "    \n",
    "    # STEP 0: AGGREGATE FACTS DATA BY CHEQUE_ID\n",
    "    print(\"\\n--- STEP 0: AGGREGATING FACTS DATA BY CHEQUE ---\")\n",
    "    \n",
    "    print(f\"Raw transaction lines: {len(facts_df):,}\")\n",
    "    print(f\"Unique cheques: {facts_df['cheque_id'].nunique():,}\")\n",
    "    print(f\"Unique customers: {facts_df['customer_id'].nunique():,}\")\n",
    "    print(f\"Unique reference numbers: {facts_df['reference_number'].nunique():,}\")\n",
    "    \n",
    "    cheque_totals = facts_df.groupby(['cheque_id', 'customer_id', 'reference_number']).agg({\n",
    "        'total_price': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    cheque_totals.columns = ['cheque_id', 'customer_id', 'reference_number', 'cheque_total']\n",
    "    \n",
    "    print(f\"Cheque-level data: {len(cheque_totals):,} records\")\n",
    "    print(f\"Average spending per cheque: ${cheque_totals['cheque_total'].mean():.2f}\")\n",
    "    \n",
    "    # STEP 1: JOIN WITH BANK DATA TO GET BANK CARD ID\n",
    "    print(f\"\\n--- STEP 1: JOINING WITH BANK DATA TO GET {bank_card_column.upper()} ---\")\n",
    "    \n",
    "    cheque_totals['reference_number'] = cheque_totals['reference_number'].astype(str)\n",
    "    bank_df['reference_number'] = bank_df['reference_number'].astype(str)\n",
    "    \n",
    "    # Select columns from bank_df\n",
    "    bank_columns = ['reference_number', bank_card_column, 'bank_name']\n",
    "    if 'client' in bank_df.columns and bank_card_column != 'client':\n",
    "        bank_columns.append('client')\n",
    "    \n",
    "    merged_df = cheque_totals.merge(\n",
    "        bank_df[bank_columns], \n",
    "        on='reference_number', \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Rename to standardize as 'bank_card_id'\n",
    "    merged_df = merged_df.rename(columns={bank_card_column: 'bank_card_id'})\n",
    "    \n",
    "    print(f\"After join:\")\n",
    "    print(f\"  Successfully merged cheque records: {len(merged_df):,}\")\n",
    "    print(f\"  Unique customers with bank data: {merged_df['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique bank cards: {merged_df['bank_card_id'].nunique():,}\")\n",
    "    print(f\"  Unique reference numbers used for linking: {merged_df['reference_number'].nunique():,}\")\n",
    "    print(f\"  Available banks: {sorted(merged_df['bank_name'].unique())}\")\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        print(\"No matching data found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # STEP 2: VALIDATE CUSTOMER-CARD COMBINATIONS\n",
    "    print(\"\\n--- STEP 2: VALIDATING CUSTOMER-CARD COMBINATIONS ---\")\n",
    "    print(\"Validation: customer_id must have >=3 cheques OR >$35 total spending per bank_card_id\")\n",
    "    \n",
    "    agg_dict = {\n",
    "        'cheque_id': 'nunique',\n",
    "        'cheque_total': 'sum',\n",
    "        'bank_name': 'first',\n",
    "        'reference_number': lambda x: list(x.unique())\n",
    "    }\n",
    "    \n",
    "    if 'client' in merged_df.columns:\n",
    "        agg_dict['client'] = 'first'\n",
    "    \n",
    "    customer_card_usage = merged_df.groupby(['customer_id', 'bank_card_id']).agg(agg_dict).reset_index()\n",
    "    \n",
    "    col_names = ['customer_id', 'bank_card_id', 'cheque_count', \n",
    "                 'total_spending', 'bank_name', 'reference_numbers']\n",
    "    if 'client' in merged_df.columns:\n",
    "        col_names.append('client')\n",
    "    \n",
    "    customer_card_usage.columns = col_names\n",
    "    \n",
    "    print(f\"Customer-card combinations: {len(customer_card_usage):,}\")\n",
    "    print(f\"Average cheques per combination: {customer_card_usage['cheque_count'].mean():.1f}\")\n",
    "    print(f\"Average spending per combination: ${customer_card_usage['total_spending'].mean():.2f}\")\n",
    "    \n",
    "    valid_customer_cards = customer_card_usage[\n",
    "        (customer_card_usage['cheque_count'] >= 3) | \n",
    "        (customer_card_usage['total_spending'] > 35)\n",
    "    ].copy()\n",
    "    \n",
    "    invalid_customer_cards = customer_card_usage[\n",
    "        (customer_card_usage['cheque_count'] < 3) & \n",
    "        (customer_card_usage['total_spending'] <= 35)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Valid customer-card combinations: {len(valid_customer_cards):,}\")\n",
    "    print(f\"Invalid customer-card combinations: {len(invalid_customer_cards):,}\")\n",
    "    print(f\"Validation rate: {len(valid_customer_cards)/len(customer_card_usage)*100:.1f}%\")\n",
    "    \n",
    "    # STEP 3: FILTER TO BOG AND TBC BANKS ONLY\n",
    "    print(\"\\n--- STEP 3: FILTERING TO BOG AND TBC BANKS ---\")\n",
    "    \n",
    "    target_banks = ['BOG', 'TBC']\n",
    "    bog_tbc_valid_cards = valid_customer_cards[\n",
    "        valid_customer_cards['bank_name'].isin(target_banks)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Valid combinations with BOG/TBC banks: {len(bog_tbc_valid_cards):,}\")\n",
    "    print(\"Bank distribution:\")\n",
    "    bank_dist = bog_tbc_valid_cards['bank_name'].value_counts()\n",
    "    for bank, count in bank_dist.items():\n",
    "        print(f\"  {bank}: {count:,} combinations\")\n",
    "    \n",
    "    if len(bog_tbc_valid_cards) == 0:\n",
    "        print(\"No valid BOG/TBC combinations found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # STEP 4: FILTER CUSTOMERS WITH <=2 VALID CARDS FROM <=2 BANKS\n",
    "    print(\"\\n--- STEP 4: FILTERING CUSTOMERS BY CARD AND BANK LIMITS ---\")\n",
    "    print(\"Requirements:\")\n",
    "    print(\"  - <=2 different valid bank_card_ids from <=2 different banks\")\n",
    "    print(\"  - If 2 cards, they MUST be from 2 different banks\")\n",
    "    \n",
    "    customer_summary = bog_tbc_valid_cards.groupby('customer_id').agg({\n",
    "        'bank_card_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': ['nunique', lambda x: list(x.unique())],\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_summary.columns = ['customer_id', 'valid_cards_count', 'valid_cards_list',\n",
    "                                'different_banks_count', 'different_banks_list', \n",
    "                                'total_cheques', 'total_spending']\n",
    "    \n",
    "    print(f\"Customers with valid BOG/TBC cards: {len(customer_summary):,}\")\n",
    "    \n",
    "    eligible_customers = customer_summary[\n",
    "        (customer_summary['valid_cards_count'] <= 2) & \n",
    "        (customer_summary['different_banks_count'] <= 2) &\n",
    "        (\n",
    "            (customer_summary['valid_cards_count'] == 1) |\n",
    "            ((customer_summary['valid_cards_count'] == 2) & (customer_summary['different_banks_count'] == 2))\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"After filtering: {len(eligible_customers):,} customers meeting requirements\")\n",
    "    \n",
    "    if len(eligible_customers) == 0:\n",
    "        print(\"No customers meet the criteria!\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"\\nValid cards per customer distribution:\")\n",
    "    cards_after = eligible_customers['valid_cards_count'].value_counts().sort_index()\n",
    "    for cards, count in cards_after.items():\n",
    "        print(f\"  {count:,} customers have {cards} valid card(s)\")\n",
    "    \n",
    "    # STEP 5: REMOVE SHARED BANK_CARD_IDS\n",
    "    print(\"\\n--- STEP 5: REMOVING SHARED BANK_CARD_IDS ---\")\n",
    "    print(\"Remove bank_card_ids used by multiple customers\")\n",
    "    \n",
    "    eligible_customer_ids = set(eligible_customers['customer_id'])\n",
    "    eligible_customer_cards = bog_tbc_valid_cards[\n",
    "        bog_tbc_valid_cards['customer_id'].isin(eligible_customer_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    card_usage_analysis = eligible_customer_cards.groupby('bank_card_id').agg({\n",
    "        'customer_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': 'first',\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    card_usage_analysis.columns = ['bank_card_id', 'customer_count', 'customer_list',\n",
    "                                   'bank_name', 'total_cheques', 'total_spending']\n",
    "    \n",
    "    shared_cards = card_usage_analysis[\n",
    "        card_usage_analysis['customer_count'] > 1\n",
    "    ]['bank_card_id'].tolist()\n",
    "    \n",
    "    unique_cards = card_usage_analysis[\n",
    "        card_usage_analysis['customer_count'] == 1\n",
    "    ]['bank_card_id'].tolist()\n",
    "    \n",
    "    print(f\"Cards used by single customer: {len(unique_cards):,}\")\n",
    "    print(f\"Cards shared by multiple customers: {len(shared_cards):,}\")\n",
    "    \n",
    "    clean_customer_cards = eligible_customer_cards[\n",
    "        ~eligible_customer_cards['bank_card_id'].isin(shared_cards)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Customer-card combinations after removing shared cards: {len(clean_customer_cards):,}\")\n",
    "    \n",
    "    # STEP 6: CREATE FINAL CLEAN DATASET\n",
    "    print(\"\\n--- STEP 6: CREATING FINAL CLEAN DATASET ---\")\n",
    "    \n",
    "    final_agg_dict = {\n",
    "        'bank_card_id': ['nunique', lambda x: list(x.unique())],\n",
    "        'bank_name': ['nunique', lambda x: list(x.unique())],\n",
    "        'cheque_count': 'sum',\n",
    "        'total_spending': 'sum',\n",
    "        'reference_numbers': lambda x: list(set([ref for refs in x for ref in refs]))\n",
    "    }\n",
    "    \n",
    "    if 'client' in clean_customer_cards.columns:\n",
    "        final_agg_dict['client'] = lambda x: list(x.unique())\n",
    "    \n",
    "    final_customer_summary = clean_customer_cards.groupby('customer_id').agg(final_agg_dict).reset_index()\n",
    "    \n",
    "    col_names = ['customer_id', 'final_cards_count', 'final_cards_list',\n",
    "                'different_banks_count', 'different_banks_list', \n",
    "                'total_cheques', 'total_spending', 'reference_numbers']\n",
    "    \n",
    "    if 'client' in clean_customer_cards.columns:\n",
    "        col_names.append('clients_list')\n",
    "    \n",
    "    final_customer_summary.columns = col_names\n",
    "    \n",
    "    final_customer_summary = final_customer_summary[\n",
    "        final_customer_summary['final_cards_count'] > 0\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Final clean customers: {len(final_customer_summary):,}\")\n",
    "    \n",
    "    constraint_violations = final_customer_summary[\n",
    "        (final_customer_summary['final_cards_count'] == 2) & \n",
    "        (final_customer_summary['different_banks_count'] != 2)\n",
    "    ]\n",
    "    \n",
    "    if len(constraint_violations) > 0:\n",
    "        print(f\"WARNING: {len(constraint_violations)} customers have 2 cards from same bank\")\n",
    "        final_customer_summary = final_customer_summary[\n",
    "            ~final_customer_summary['customer_id'].isin(constraint_violations['customer_id'])\n",
    "        ].copy()\n",
    "        print(f\"Removed violations, final customers: {len(final_customer_summary):,}\")\n",
    "    \n",
    "    print(\"\\nFINAL DISTRIBUTION - Cards per customer:\")\n",
    "    final_cards_dist = final_customer_summary['final_cards_count'].value_counts().sort_index()\n",
    "    for cards, count in final_cards_dist.items():\n",
    "        print(f\"  {count:,} customers have {cards} card(s)\")\n",
    "    \n",
    "    # Final verification\n",
    "    max_cards = final_customer_summary['final_cards_count'].max()\n",
    "    max_banks = final_customer_summary['different_banks_count'].max()\n",
    "    two_cards_same_bank = len(final_customer_summary[\n",
    "        (final_customer_summary['final_cards_count'] == 2) & \n",
    "        (final_customer_summary['different_banks_count'] == 1)\n",
    "    ])\n",
    "    \n",
    "    final_customer_ids = set(final_customer_summary['customer_id'])\n",
    "    final_clean_cards = clean_customer_cards[\n",
    "        clean_customer_cards['customer_id'].isin(final_customer_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    final_card_sharing = final_clean_cards.groupby('bank_card_id')['customer_id'].nunique()\n",
    "    shared_in_final = (final_card_sharing > 1).sum()\n",
    "    \n",
    "    print(f\"\\nFINAL VERIFICATION:\")\n",
    "    print(f\"  Maximum cards per customer: {max_cards} (requirement: <=2) {'PASS' if max_cards <= 2 else 'FAIL'}\")\n",
    "    print(f\"  Maximum banks per customer: {max_banks} (requirement: <=2) {'PASS' if max_banks <= 2 else 'FAIL'}\")\n",
    "    print(f\"  Customers with 2 cards from same bank: {two_cards_same_bank} (requirement: 0) {'PASS' if two_cards_same_bank == 0 else 'FAIL'}\")\n",
    "    print(f\"  Shared cards in final dataset: {shared_in_final} (requirement: 0) {'PASS' if shared_in_final == 0 else 'FAIL'}\")\n",
    "    \n",
    "    all_passed = max_cards <= 2 and max_banks <= 2 and two_cards_same_bank == 0 and shared_in_final == 0\n",
    "    print(f\"  OVERALL: {'ALL REQUIREMENTS SATISFIED' if all_passed else 'REQUIREMENTS NOT MET'}\")\n",
    "    \n",
    "    # CREATE CUSTOMER-CARD PAIRS OUTPUT\n",
    "    print(\"\\n--- CREATING CUSTOMER-CARD PAIRS ---\")\n",
    "    \n",
    "    customer_card_pairs = final_clean_cards[['customer_id', 'bank_card_id', 'bank_name']].drop_duplicates()\n",
    "    print(f\"Total customer-card pairs: {len(customer_card_pairs):,}\")\n",
    "    \n",
    "    pairs_per_customer = customer_card_pairs.groupby('customer_id').size()\n",
    "    print(f\"Pairs per customer: min={pairs_per_customer.min()}, max={pairs_per_customer.max()}\")\n",
    "    \n",
    "    results = {\n",
    "        'raw_transaction_lines': len(facts_df),\n",
    "        'total_cheques': len(cheque_totals),\n",
    "        'total_loyalty_customers': cheque_totals['customer_id'].nunique(),\n",
    "        'customers_with_bank_data': merged_df['customer_id'].nunique(),\n",
    "        'total_customer_card_combinations': len(customer_card_usage),\n",
    "        'valid_customer_card_combinations': len(valid_customer_cards),\n",
    "        'bog_tbc_valid_combinations': len(bog_tbc_valid_cards),\n",
    "        'eligible_customers_before_shared_removal': len(eligible_customers),\n",
    "        'shared_cards_count': len(shared_cards),\n",
    "        'final_clean_customers': len(final_customer_summary),\n",
    "        'final_clean_combinations': len(final_clean_cards),\n",
    "        'bank_card_column_used': bank_card_column,\n",
    "        \n",
    "        'cheque_totals': cheque_totals,\n",
    "        'customer_card_usage': customer_card_usage,\n",
    "        'valid_customer_cards': valid_customer_cards,\n",
    "        'invalid_customer_cards': invalid_customer_cards,\n",
    "        'bog_tbc_valid_cards': bog_tbc_valid_cards,\n",
    "        'eligible_customers': eligible_customers,\n",
    "        'card_usage_analysis': card_usage_analysis,\n",
    "        'shared_cards_info': card_usage_analysis[card_usage_analysis['customer_count'] > 1],\n",
    "        'shared_cards_list': shared_cards,\n",
    "        'final_clean_cards': final_clean_cards,\n",
    "        'final_customer_summary': final_customer_summary,\n",
    "        'customer_card_pairs': customer_card_pairs,\n",
    "        'merged_data': merged_df,\n",
    "        'final_cards_distribution': final_cards_dist\n",
    "    }\n",
    "    \n",
    "    return results, final_customer_summary\n",
    "\n",
    "def save_corrected_results(results, output_dir=\"./data\"):\n",
    "    \"\"\"Save all results to CSV files\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    try:\n",
    "        bank_card_col = results.get('bank_card_column_used', 'client')\n",
    "        print(f\"\\nNote: Bank card column in original data was '{bank_card_col}'\")\n",
    "        print(f\"Outputs will show this as 'bank_card_id'\\n\")\n",
    "        \n",
    "        # PRIMARY OUTPUT: Customer-Card Pairs\n",
    "        if 'customer_card_pairs' in results and not results['customer_card_pairs'].empty:\n",
    "            file_path = output_path / \"customer_card_pairs.csv\"\n",
    "            results['customer_card_pairs'].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "            print(f\"PRIMARY OUTPUT SAVED: {file_path}\")\n",
    "            print(f\"  Rows: {len(results['customer_card_pairs']):,}\")\n",
    "            print(f\"  Columns: {list(results['customer_card_pairs'].columns)}\")\n",
    "        \n",
    "        # Customer summary\n",
    "        if 'final_customer_summary' in results and not results['final_customer_summary'].empty:\n",
    "            summary_csv = results['final_customer_summary'].copy()\n",
    "            \n",
    "            summary_csv['final_cards_list_str'] = summary_csv['final_cards_list'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            summary_csv['different_banks_list_str'] = summary_csv['different_banks_list'].apply(\n",
    "                lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            if 'clients_list' in summary_csv.columns:\n",
    "                summary_csv['clients_list_str'] = summary_csv['clients_list'].apply(\n",
    "                    lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            summary_csv['reference_numbers_str'] = summary_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x[:20])) + (f' (+{len(x)-20} more)' if len(x) > 20 else '') if isinstance(x, list) else str(x))\n",
    "            \n",
    "            csv_columns = ['customer_id', 'final_cards_count', 'different_banks_count', 'total_cheques', \n",
    "                          'total_spending', 'final_cards_list_str', 'different_banks_list_str', 'reference_numbers_str']\n",
    "            \n",
    "            if 'clients_list_str' in summary_csv.columns:\n",
    "                csv_columns.insert(-1, 'clients_list_str')\n",
    "            \n",
    "            file_path = output_path / \"final_clean_customers_summary.csv\"\n",
    "            summary_csv[csv_columns].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Detailed combinations\n",
    "        if 'final_clean_cards' in results and not results['final_clean_cards'].empty:\n",
    "            clean_cards_csv = results['final_clean_cards'].copy()\n",
    "            clean_cards_csv['reference_numbers_str'] = clean_cards_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            output_cols = ['customer_id', 'bank_card_id', 'bank_name', 'cheque_count', \n",
    "                          'total_spending', 'reference_numbers_str']\n",
    "            \n",
    "            if 'client' in clean_cards_csv.columns:\n",
    "                output_cols.insert(3, 'client')\n",
    "            \n",
    "            file_path = output_path / \"customer_card_combinations_detailed.csv\"\n",
    "            clean_cards_csv[output_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Shared cards\n",
    "        if 'shared_cards_info' in results and not results['shared_cards_info'].empty:\n",
    "            file_path = output_path / \"shared_cards_removed.csv\"\n",
    "            shared_csv = results['shared_cards_info'].copy()\n",
    "            shared_csv['customer_list_str'] = shared_csv['customer_list'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            csv_cols = ['bank_card_id', 'customer_count', 'bank_name', 'total_cheques', \n",
    "                       'total_spending', 'customer_list_str']\n",
    "            shared_csv[csv_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Invalid combinations\n",
    "        if 'invalid_customer_cards' in results and not results['invalid_customer_cards'].empty:\n",
    "            file_path = output_path / \"invalid_customer_card_combinations.csv\"\n",
    "            invalid_csv = results['invalid_customer_cards'].copy()\n",
    "            invalid_csv['reference_numbers_str'] = invalid_csv['reference_numbers'].apply(\n",
    "                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "            \n",
    "            output_cols = ['customer_id', 'bank_card_id', 'cheque_count', 'total_spending', \n",
    "                          'bank_name', 'reference_numbers_str']\n",
    "            invalid_csv[output_cols].to_csv(file_path, index=False)\n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # Summary\n",
    "        analysis_summary = {\n",
    "            'total_loyalty_customers': results['total_loyalty_customers'],\n",
    "            'final_clean_customers': results['final_clean_customers'],\n",
    "            'success_rate_percent': results['final_clean_customers']/results['total_loyalty_customers']*100,\n",
    "            'shared_cards_removed': results['shared_cards_count'],\n",
    "            'validation_rate_percent': results['valid_customer_card_combinations']/results['total_customer_card_combinations']*100,\n",
    "            'bank_card_column_in_original_data': results.get('bank_card_column_used', 'client')\n",
    "        }\n",
    "        \n",
    "        file_path = output_path / \"analysis_summary.csv\"\n",
    "        pd.DataFrame([analysis_summary]).to_csv(file_path, index=False)\n",
    "        saved_files.append(str(file_path))\n",
    "        \n",
    "        print(f\"\\nSAVED {len(saved_files)} FILES:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"   {file_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def run_full_analysis():\n",
    "    \"\"\"Run complete analysis using PRESAVED data files\"\"\"\n",
    "    print(\"RUNNING COMPLETE ANALYSIS - WITH CLIENT AS BANK CARD ID\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Looking for presaved data files...\")\n",
    "    \n",
    "    facts_df = load_dataframe(\"facts_data_2025_combined_customer_id\")\n",
    "    bank_df = load_dataframe(\"bank_data_combined\")\n",
    "    \n",
    "    if facts_df is None or bank_df is None:\n",
    "        print(\"\\nERROR: No presaved data found!\")\n",
    "        print(\"Expected files in './analysis_data/' directory:\")\n",
    "        print(\"  - facts_data_2025_combined_customer_id.parquet (or .pkl)\")\n",
    "        print(\"  - bank_data_combined.parquet (or .pkl)\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nSUCCESS: Found presaved data!\")\n",
    "    print(f\"  Facts data: {len(facts_df):,} records\")\n",
    "    print(f\"  Bank data: {len(bank_df):,} records\")\n",
    "    \n",
    "    results, summary = analyze_loyalty_bank_cards_corrected(facts_df, bank_df)\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"Analysis failed\")\n",
    "        return None\n",
    "    \n",
    "    save_corrected_results(results)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "def run_analysis_with_dataframes(facts_df, bank_df):\n",
    "    \"\"\"Run analysis using dataframes you already have loaded\"\"\"\n",
    "    print(\"RUNNING ANALYSIS WITH PROVIDED DATAFRAMES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if facts_df is None or bank_df is None:\n",
    "        print(\"ERROR: One or both dataframes are None\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Facts data: {len(facts_df):,} records\")\n",
    "    print(f\"Bank data: {len(bank_df):,} records\")\n",
    "    \n",
    "    results, summary = analyze_loyalty_bank_cards_corrected(facts_df, bank_df)\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"Analysis failed\")\n",
    "        return None\n",
    "    \n",
    "    save_corrected_results(results)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "def show_usage():\n",
    "    print(\"\"\"\n",
    "USAGE - CORRECTED VERSION WITH CLIENT AS BANK CARD NUMBER\n",
    "\n",
    "==================================================================\n",
    "VIEW YOUR DATA FIRST (RECOMMENDED)\n",
    "==================================================================\n",
    ">>> bank_df = view_bank_data()\n",
    ">>> facts_df = view_facts_data()\n",
    "\n",
    "==================================================================\n",
    "RUN ANALYSIS\n",
    "==================================================================\n",
    "Option 1 - Use presaved data:\n",
    ">>> results, summary = run_full_analysis()\n",
    "\n",
    "Option 2 - Use dataframes in memory:\n",
    ">>> results, summary = run_analysis_with_dataframes(facts_df, bank_df)\n",
    "\n",
    "==================================================================\n",
    "OUTPUT FILES\n",
    "==================================================================\n",
    "Primary: customer_card_pairs.csv (customer_id, bank_card_id, bank_name)\n",
    "\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    show_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59343ec8-d22b-4f98-a4c7-10a4daf35836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING COMPLETE ANALYSIS - WITH CLIENT AS BANK CARD ID\n",
      "============================================================\n",
      "Looking for presaved data files...\n",
      "\n",
      "SUCCESS: Found presaved data!\n",
      "  Facts data: 24,519,920 records\n",
      "  Bank data: 35,362,470 records\n",
      "\n",
      "=== CORRECTED LOYALTY BANK CARD ANALYSIS ===\n",
      "\n",
      "--- DIAGNOSTIC: CHECKING AVAILABLE COLUMNS ---\n",
      "Facts columns: ['cheque_id', 'is_loyalty', 'discount_card_no', 'customer_id', 'reference_number', 'total_price', 'item_count']\n",
      "Bank columns: ['reference_number', 'transaction_amount', 'bank_name', 'client']\n",
      "\n",
      "Bank data sample (first row):\n",
      "[{'reference_number': '522310265367', 'transaction_amount': Decimal('10.98'), 'bank_name': 'BOG', 'client': '5e2df56dbb9a6573cff6180d6c0a21d05fb1ca2e9c9121a7f5275e6afd853415'}]\n",
      "\n",
      "Using 'client' as bank card identifier\n",
      "\n",
      "--- STEP 0: AGGREGATING FACTS DATA BY CHEQUE ---\n",
      "Raw transaction lines: 24,519,920\n",
      "Unique cheques: 24,519,797\n",
      "Unique customers: 691,124\n",
      "Unique reference numbers: 24,477,555\n",
      "Cheque-level data: 24,519,920 records\n",
      "Average spending per cheque: $17.82\n",
      "\n",
      "--- STEP 1: JOINING WITH BANK DATA TO GET CLIENT ---\n",
      "After join:\n",
      "  Successfully merged cheque records: 17,745,787\n",
      "  Unique customers with bank data: 625,579\n",
      "  Unique bank cards: 1,115,904\n",
      "  Unique reference numbers used for linking: 17,625,116\n",
      "  Available banks: ['BOG', 'TBC']\n",
      "\n",
      "--- STEP 2: VALIDATING CUSTOMER-CARD COMBINATIONS ---\n",
      "Validation: customer_id must have >=3 cheques OR >$35 total spending per bank_card_id\n",
      "Customer-card combinations: 1,603,667\n",
      "Average cheques per combination: 11.1\n",
      "Average spending per combination: $197.09\n",
      "Valid customer-card combinations: 915,907\n",
      "Invalid customer-card combinations: 687,760\n",
      "Validation rate: 57.1%\n",
      "\n",
      "--- STEP 3: FILTERING TO BOG AND TBC BANKS ---\n",
      "Valid combinations with BOG/TBC banks: 915,907\n",
      "Bank distribution:\n",
      "  BOG: 588,788 combinations\n",
      "  TBC: 327,119 combinations\n",
      "\n",
      "--- STEP 4: FILTERING CUSTOMERS BY CARD AND BANK LIMITS ---\n",
      "Requirements:\n",
      "  - <=2 different valid bank_card_ids from <=2 different banks\n",
      "  - If 2 cards, they MUST be from 2 different banks\n",
      "Customers with valid BOG/TBC cards: 513,003\n",
      "After filtering: 360,469 customers meeting requirements\n",
      "\n",
      "Valid cards per customer distribution:\n",
      "  277,154 customers have 1 valid card(s)\n",
      "  83,315 customers have 2 valid card(s)\n",
      "\n",
      "--- STEP 5: REMOVING SHARED BANK_CARD_IDS ---\n",
      "Remove bank_card_ids used by multiple customers\n",
      "Cards used by single customer: 407,339\n",
      "Cards shared by multiple customers: 17,852\n",
      "Customer-card combinations after removing shared cards: 407,339\n",
      "\n",
      "--- STEP 6: CREATING FINAL CLEAN DATASET ---\n",
      "Final clean customers: 335,846\n",
      "\n",
      "FINAL DISTRIBUTION - Cards per customer:\n",
      "  264,353 customers have 1 card(s)\n",
      "  71,493 customers have 2 card(s)\n",
      "\n",
      "FINAL VERIFICATION:\n",
      "  Maximum cards per customer: 2 (requirement: <=2) PASS\n",
      "  Maximum banks per customer: 2 (requirement: <=2) PASS\n",
      "  Customers with 2 cards from same bank: 0 (requirement: 0) PASS\n",
      "  Shared cards in final dataset: 0 (requirement: 0) PASS\n",
      "  OVERALL: ALL REQUIREMENTS SATISFIED\n",
      "\n",
      "--- CREATING CUSTOMER-CARD PAIRS ---\n",
      "Total customer-card pairs: 407,339\n",
      "Pairs per customer: min=1, max=2\n",
      "\n",
      "Note: Bank card column in original data was 'client'\n",
      "Outputs will show this as 'bank_card_id'\n",
      "\n",
      "PRIMARY OUTPUT SAVED: data\\customer_card_pairs.csv\n",
      "  Rows: 407,339\n",
      "  Columns: ['customer_id', 'bank_card_id', 'bank_name']\n",
      "\n",
      "SAVED 6 FILES:\n",
      "   data\\customer_card_pairs.csv\n",
      "   data\\final_clean_customers_summary.csv\n",
      "   data\\customer_card_combinations_detailed.csv\n",
      "   data\\shared_cards_removed.csv\n",
      "   data\\invalid_customer_card_combinations.csv\n",
      "   data\\analysis_summary.csv\n"
     ]
    }
   ],
   "source": [
    "results, summary = run_full_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb59a96-df8c-4b17-8dd7-274a0d620982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a22fb61-6f2a-400c-bee5-23cdfd3a9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #show_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d595fb-f5d2-423f-81de-f9f62ad7b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d3f6da-832f-445c-b544-b6c34aad60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_all_data_safely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e044f3b-5541-4c2a-a43c-a4a9d56ee573",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_complete_analysis_with_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results, summary \u001b[38;5;241m=\u001b[39m run_complete_analysis_with_validation()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_complete_analysis_with_validation' is not defined"
     ]
    }
   ],
   "source": [
    "results, summary = run_complete_analysis_with_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce8a79-04a6-457d-99b9-d941f77f6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTS DATA - IS_LOYALTY BREAKDOWN\n",
      "============================================================\n",
      "Total rows: 24,519,920\n",
      "\n",
      "is_loyalty breakdown:\n",
      "is_loyalty\n",
      "1    24519920\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "is_loyalty\n",
      "1    100.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- NON-LOYALTY TRANSACTIONS (is_loyalty = False) ---\n",
      "Total non-loyalty rows: 0\n",
      "Non-loyalty with customer_id (should be 0): 0\n",
      "Non-loyalty without customer_id (expected): 0\n",
      "\n",
      "--- LOYALTY TRANSACTIONS (is_loyalty = True) ---\n",
      "Total loyalty rows: 24,519,920\n",
      "Loyalty with customer_id: 24,519,920\n",
      "Unique loyalty customers: 691,124\n",
      "\n",
      "--- SAMPLE NON-LOYALTY TRANSACTION ---\n",
      "Empty DataFrame\n",
      "Columns: [cheque_id, is_loyalty, discount_card_no, customer_id, reference_number, total_price, item_count]\n",
      "Index: []\n",
      "\n",
      "--- SAMPLE LOYALTY TRANSACTION ---\n",
      "   cheque_id  is_loyalty discount_card_no               customer_id  \\\n",
      "0  622115623           1     888800035608  6084819b2c5031000fbcf349   \n",
      "\n",
      "  reference_number total_price  item_count  \n",
      "0     500407689483       16.95           5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the facts data\n",
    "DATA_DIR = Path(\"analysis_data\")\n",
    "facts_df = pd.read_parquet(DATA_DIR / \"facts_data_2025_combined_customer_id.parquet\")\n",
    "\n",
    "# Check the is_loyalty column\n",
    "print(\"FACTS DATA - IS_LOYALTY BREAKDOWN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(facts_df):,}\")\n",
    "\n",
    "if 'is_loyalty' in facts_df.columns:\n",
    "    print(f\"\\nis_loyalty breakdown:\")\n",
    "    print(facts_df['is_loyalty'].value_counts())\n",
    "    print(f\"\\nPercentages:\")\n",
    "    print(facts_df['is_loyalty'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Check customer_id for non-loyalty transactions\n",
    "    print(f\"\\n--- NON-LOYALTY TRANSACTIONS (is_loyalty = False) ---\")\n",
    "    non_loyalty = facts_df[facts_df['is_loyalty'] == False]\n",
    "    print(f\"Total non-loyalty rows: {len(non_loyalty):,}\")\n",
    "    print(f\"Non-loyalty with customer_id (should be 0): {non_loyalty['customer_id'].notna().sum():,}\")\n",
    "    print(f\"Non-loyalty without customer_id (expected): {non_loyalty['customer_id'].isna().sum():,}\")\n",
    "    \n",
    "    print(f\"\\n--- LOYALTY TRANSACTIONS (is_loyalty = True) ---\")\n",
    "    loyalty = facts_df[facts_df['is_loyalty'] == True]\n",
    "    print(f\"Total loyalty rows: {len(loyalty):,}\")\n",
    "    print(f\"Loyalty with customer_id: {loyalty['customer_id'].notna().sum():,}\")\n",
    "    print(f\"Unique loyalty customers: {loyalty['customer_id'].nunique():,}\")\n",
    "    \n",
    "    # Show sample of each\n",
    "    print(f\"\\n--- SAMPLE NON-LOYALTY TRANSACTION ---\")\n",
    "    print(non_loyalty.head(1))\n",
    "    \n",
    "    print(f\"\\n--- SAMPLE LOYALTY TRANSACTION ---\")\n",
    "    print(loyalty.head(1))\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: 'is_loyalty' column not found!\")\n",
    "    print(f\"Available columns: {list(facts_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd9f33-b80c-4cab-a5ea-e326390cc4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b3670-0568-43c8-ade2-37e686513634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8431a5-e507-4854-80b7-2c789bae5161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f30b1-96d0-4637-ba2f-ecdeedb919e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c09db-dac3-4901-88e7-3fef4a5a756c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529cf05-feb5-4279-9c75-37c59c4a97e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acc34d-914a-44a4-9e4f-6998085614d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610ed2c-52ce-44b9-bdf0-8e4a7b79a646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bed75b-4bfb-44f6-b13f-293916f5f072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adbf0c-db59-4898-a6eb-9be64a6126f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634c2d3-0d18-4b08-a579-ba50814fd183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf638ec-1011-4056-9701-7b87a9aa6b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6385ad-5cb0-44ed-a94f-d99cb6ad5c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc0bec-b054-45bc-8272-45e521567c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97e659-9500-4c40-aa7d-827431eafc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
